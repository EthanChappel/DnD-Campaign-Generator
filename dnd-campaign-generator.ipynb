{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "text_generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python391jvsc74a57bd0b29f710a3fe0887ef7e98cfa09b5eddf968cb1901ab7bec13836ac29292d7b2d",
      "display_name": "Python 3.9.1 64-bit"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srXC6pLGLwS6"
      },
      "source": [
        "# Dungeons and Dragons Campaign Generator\n",
        "Ethan Chappel (mxk273), Nathan Mauch (nja859), Franscisco Perales (ffn480)\n",
        "\n",
        "This model was built with help from the TensorFlow [Text generation with an RNN](https://www.tensorflow.org/tutorials/text/text_generation) tutorial.\n",
        "\n",
        "## Setup\n",
        "\n",
        "### Import libraries and set constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yG_n40gFzf9s"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import glob\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "\n",
        "EPOCHS = 50\n",
        "OUTPUT_LENGTH = 5000\n",
        "MODEL_TEMPERATURE = 1.0\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "START_STRING = 'Dungeons & Dragons\\n'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "### Retrieve the dataset"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aavnuByVymwK",
        "tags": []
      },
      "source": [
        "text = ''\n",
        "for fp in glob.iglob('data/*.txt'):\n",
        "    # Read in the text:\n",
        "    print(fp)\n",
        "    with open(fp, 'r') as f:\n",
        "        text += f'{f.read()}\\n\\n'\n",
        "\n",
        "# Count the unique characters in the file\n",
        "vocabulary = sorted(set(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNnrKn_lL-IJ"
      },
      "source": [
        "## Process the input text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wd2m3mqkDjRj"
      },
      "source": [
        "# Vectorize text\n",
        "example_texts = ['abcdefg', 'xyz']\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "\n",
        "# Create the StringLookup layer.\n",
        "ids_from_chars = preprocessing.StringLookup(vocabulary=list(vocabulary))\n",
        "ids = ids_from_chars(chars)\n",
        "\n",
        "chars_from_ids = preprocessing.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(),\n",
        "    invert=True\n",
        ")\n",
        "\n",
        "# This layer recovers characters from the vectors of IDs and returns them as a RaggedTensor of characters:\n",
        "chars = chars_from_ids(ids)\n",
        "\n",
        "# Join the characters back into strings. \n",
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgsVvVxnymwf"
      },
      "source": [
        "### Prediction task\n",
        "\n",
        "#### Create training examples and targets\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UopbsKi88tm5"
      },
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "\n",
        "# Convert the text vector into a stream of indices.\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "\n",
        "sequence_length = 100\n",
        "examples_per_epoch = len(text) // (sequence_length + 1)\n",
        "\n",
        "# Convert these individual characters to sequences of the desired size.\n",
        "sequences = ids_dataset.batch(sequence_length + 1, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NGu-FkO_kYU"
      },
      "source": [
        "# Takes a sequence as input, duplicates, and shifts it to align the input and label for each timestep:\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJdfPmdqzf-R"
      },
      "source": [
        "### Create training batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2pGotuNzf-S"
      },
      "source": [
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6oUuElIMgVx"
      },
      "source": [
        "## Build The Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wj8HQ2w8z4iO"
      },
      "source": [
        "class GruModel(tf.keras.Model):\n",
        "  def __init__(self, vocabulary_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocabulary_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units, return_sequences=True, return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocabulary_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    i = inputs\n",
        "    i = self.embedding(i, training=training)\n",
        "    if states is None:\n",
        "        states = self.gru.get_initial_state(i)\n",
        "    i, states = self.gru(i, initial_state=states, training=training)\n",
        "    i = self.dense(i, training=training)\n",
        "\n",
        "    if return_state:\n",
        "        return i, states\n",
        "    else:\n",
        "        return i\n",
        "\n",
        "\n",
        "model = GruModel(len(ids_from_chars.get_vocabulary()), EMBEDDING_DIM, RNN_UNITS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ubPo0_9Prjb"
      },
      "source": [
        "## Try the model\n",
        "\n",
        "Check the output's shape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-_70kKAPrPU"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4V4MfFg0RQJg"
      },
      "source": [
        "# Gives us a prediction of the next character index at each timestep\n",
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJL0Q0YPY6Ee"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HrXTACTdzY-"
      },
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "mean_loss = example_batch_loss.numpy().mean()\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", mean_loss)\n",
        "\n",
        "# Configure the training procedure.\n",
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ky3F_BhgkTW"
      },
      "source": [
        "### Execute training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yGBE2zxMMHs"
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKkD5M6eoSiN"
      },
      "source": [
        "## Generate text\n",
        "\n",
        "This class makes a single step prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSBU1tHmlUSs"
      },
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"\" or \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['', '[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        values=[-float('inf')] * len(skip_ids),  # Put a -inf at each bad index.\n",
        "        indices=skip_ids,\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())]  # Match the shape to the vocabulary\n",
        "    )\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(\n",
        "        inputs=input_ids,\n",
        "        states=states,\n",
        "        return_state=True,\n",
        "    )\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits / self.temperature\n",
        "    # Apply the prediction mask: prevent \"\" or \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states\n",
        "\n",
        "\n",
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars, MODEL_TEMPERATURE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "### Generate text for the number of characters defined by `OUTPUT_LENGTH`"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ST7PSyk9t1mT"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant([START_STRING])\n",
        "result = [next_char]\n",
        "\n",
        "# Generate text with the model.\n",
        "for n in range(OUTPUT_LENGTH):\n",
        "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "    result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}